## 1. 데이터 내 단어 빈도수 계산

모든 데이터 분석에 앞서 더 정확한 모델링을 위해 데이터의 특징을 살펴보는 것이 중요합니다. 텍스트의 경우, 데이터 내 단어의 빈도수를 살펴보는 것으로 특징을 파악할 수 있습니다.

이번 실습에서는 영화 리뷰 데이터인 `IMDB dataset`에서 단어별 빈도수를 살펴볼 예정입니다.

## 지시사항

1. `IMDB dataset`이 들어 있는 `text.txt` 파일을 확인해 봅니다. 파일 내 각 줄은 하나의 리뷰에 해당합니다.
2. 텍스트 데이터를 불러오면서 **단어**가 key, **빈도수**가 value로 구성된 딕셔너리 변수인 `word_counter`를 만드세요.
   - 파일 내 각 줄 끝에는 새로운 줄을 의미하는 특수기호(`\n`)가 추가되어 있습니다. `rstrip()` 함수를 이용하여 각 줄 맨 끝에 있는 특수기호를 제거하세요.
   - `split()` 함수를 사용하면 각 줄을 공백 기준으로 분리하여 단어를 추출할 수 있습니다.
3. `word_counter`를 활용하여, `text.txt`에 들어 있는 **모든 단어의 빈도수 총합**을 `total` 변수에 저장하세요.
4. `word_counter`를 활용하여, `text.txt` 내 **100회 이상 발생하는 단어**를 `up_five` 리스트에 저장하세요.



## 2. 텍스트 전처리를 통한 데이터 탐색

모든 텍스트 분석에 앞서 텍스트 안에 어떠한 단어가 존재하는지 살펴보는 것이 중요합니다.
하지만 단순히 텍스트를 공백 기준으로 나눠 단어를 추출하면 여러 가지 문제점이 발생합니다.

동일한 의미를 가진 단어를 여러 방식으로 표현하여 사용하기 때문입니다. 예를 들어, computer라는 단어 또한 문장 내 위치에 따라 Computer와 같이 **대문자**로 표기하거나, computer.와 같이 **특수기호**와 함께 표기되기도 합니다.

이번 실습에서는 **대소문자** 및 **특수기호**를 제거하는 텍스트 전처리를 통해 데이터를 탐색해볼 것입니다.

## 지시사항

1. 영화 리뷰를 불러오면서 모든 리뷰를 **소문자 처리**를 하고, 단어 내 알파벳을 제외한 모든 **숫자 및 특수기호**를 제거해 주세요.
   - `문자열.lower()`: 해당 문자열을 모두 소문자로 변환할 수 있습니다.
   - `regex.sub('', 문자열)`: 문자열 내 regex 변수의 정규표현식에 해당하는 모든 문자를 제거(”로 교체)
   - 전처리가 완료된 단어와 단어의 빈도수를 `word_counter` 딕셔너리에 저장하세요.
2. `test.txt`에 존재하는 단어 `the`의 빈도수를 `count` 변수에 저장하세요.



## 3. NLTK를 통한 stopwords 및 stemming 처리

**NLTK(Natural Language Toolkit)** 은 텍스트 전처리 및 탐색 코드를 보다 빠르고 간편하게 작성할 수 있게 도와주는 Python 라이브러리입니다.

이번 실습에서는 NLTK를 활용하여, 문서의 여러 통계치를 계산하고 전처리된 데이터를 저장하는 실습을 진행해 보겠습니다.

## 지시사항

1. NLTK에서 기본적으로 제공되는 **영어 stopword** 를 `stopwords` 변수에 저장하세요.
2. `new_keywords` 리스트에 저장되어 있는 신규 stopword들을 1번에서 정의한 `stopwords` 변수에 추가하여 `updated_stopwords`에 저장해주세요.
3. `test_sentences` 내 각 문장을 단어 기준으로 토큰화 해주세요. 토큰화를 수행하면서 stopword에 해당되는 단어는 제거하고, 각 문장별 결과를 `tokenized_word` 리스트에 추가하세요. (이번 실습에서는 `nltk`의 함수인 `word_tokenize`를 통해 입력되는 문자열을 토큰화하고 있습니다)
4. `PorterStemmer`를 사용하여 토큰화된 test_sentences가 들어 있는 `tokenized_word`의 **첫 문장**에 stemming을 수행하고 결과를 `stemmed_sent` 리스트에 추가하세요.



## 4. word2vec으로 단어 유사도 측정

word2vec은 신경망을 통해 **단어 임베딩 벡터**를 학습합니다. 이번 실습에서는 파이썬 라이브러리인 `gensim`을 사용하여 word2vec을 학습하도록 하겠습니다.

학습데이터로는 개인의 감정을 표현하는 문장으로 구성된 `Emotions dataset for NLP` 데이터셋을 사용할 예정입니다.

## 지시사항

1. `Emotions dataset for NLP` 데이터셋을 불러오는 `load_data` 함수는 이미 작성되어 있습니다.
2. `input_data`에 저장되어 있는 텍스트 데이터를 사용해서 단어별 문맥의 길이를 의미하는 `window`는 2, 벡터의 차원이 300인 word2vec 모델을 학습하세요. (`epochs`는 10으로 설정)
3. 단어 **happy**와 유사한 단어 10개를 `similar_happy` 변수에 저장하세요.
4. 단어 **sad**와 유사한 단어 10개를 `similar_sad` 변수에 저장하세요.
5. **good**과 **bad**의 임베딩 벡터 간 유사도를 `similar_good_bad` 변수에 저장하세요.
6. **sad**와 **lonely**의 임베딩 벡터 간 유사도를 `similar_sad_lonely` 변수에 저장하세요.
7. **happy**의 임베딩 벡터를 `wv_happy` 변수에 저장하세요.



## 5.fastText로 단어 임베딩 벡터 생성

**fastText**는 word2vec의 단점인 미등록 단어 문제를 해결합니다. 이번 실습에서는 파이썬 라이브러리인 **gensim**을 사용하여 fastText를 학습하도록 하겠습니다.

학습 데이터로는 개인의 감정을 표현하는 문장으로 구성된 `Emotions dataset for NLP` 데이터셋을 사용하겠습니다.

## 지시사항

1. `input_data`에 저장되어 있는 텍스트 데이터를 사용해서 단어별 문맥의 길이를 의미하는 `window`는 3, 벡터의 차원이 100, 단어의 최소 발생 빈도를 의미하는 `min_count`가 10인 fastText 모델을 학습하세요.
   - `epochs`는 10으로 설정합니다.
2. 단어 **day**와 유사한 단어 10개를 `similar_day` 변수에 저장하세요.
3. 단어 **night**와 유사한 단어 10개를 `similar_night` 변수에 저장하세요.
4. **elllllllice**의 임베딩 벡터를 `wv_elice` 변수에 저장하세요.



## 6. 모델 학습을 위한 데이터 분할

이번 과정에서는 감정 분석 모델을 만들고 활용하는 방법을 배워보도록 하겠습니다. 본격적으로 모델을 만들어 보기에 앞서 주어진 데이터를 **학습 데이터**와 **평가 데이터**로 나누는 방법에 대해서 배워보도록 하겠습니다.

**학습 데이터**란 감정 분석 모델을 훈련 시키기 위해 문장과 해당 문장의 감정이 포함되어 있는 데이터셋을 의미합니다.

**평가 데이터**란 학습된 모델의 성능을 평가하기 위해 학습에 포함되지 않은 데이터셋을 의미합니다.

이번 과정에서 `Emotions dataset for NLP` 데이터셋을 활용하여 **문장별 감정 분석**을 진행해 볼 예정입니다. 본 데이터셋의 각 줄은 아래와 같이 `문장;감정`의 형태로 구성이 되어 있습니다.

```
i didnt feel humiliated;sadness
```

## 지시사항

1. `emotions_train.txt`에 들어 있는 데이터를 불러와서 각 줄을 `(문장, 감정)` 형태의 튜플로 `data` 리스트에 저장하세요.
   - 각 줄에서 문장과 감정은 `;`으로 구분되어 있다는 점에 유의하세요.
   - 모든 줄의 끝에 존재하는 `\n`은 제거하세요.
2. `scikit-learn`의 `train_test_split` 함수를 사용하여, 학습 데이터와 평가 데이터를 8:2의 기준으로 분할하세요.
   - 매개변수에서 `test_size`는 0.2, `random_state`는 7로 설정하세요.
3. 학습 데이터의 문장을 `Xtrain` 변수에, 감정을 `Ytrain` 변수에 저장하세요.
4. 학습 데이터 내 **문장의 개수**와 **감정의 종류**를 출력하세요.
5. 평가 데이터의 문장을 `Xtest` 변수에 감정을 `Ytest` 변수에 저장하세요.
6. 평가 데이터 내 **문장의 개수**를 출력하세요.



## 7. 나이브 베이즈 학습

나이브 베이즈 기법에서는 각 감정 내 단어의 **가능도(likelihood)** 를 기반으로 문장의 감정을 예측합니다. 감정 내 단어의 가능도는 아래와 같은 공식으로 계산을 할 수 있습니다.

P^(단어|감정)=감정 내 단어의 빈도수감정 내 모든 단어의 빈도수*P*^(**단어****|****감정**)=**감정** **내** **모든** **단어의** **빈도수****감정** **내** **단어의** **빈도수**

이번 실습에서는 먼저 단어들의 가능도를 구하는 함수를 작성하도록 하겠습니다. 학습 데이터로 `Emotions dataset for NLP`데이터셋을 사용합니다.

## 지시사항

1. `cal_partial_freq()` 함수를 완성하세요. 함수는 텍스트 데이터(`texts`)와 특정 감정(`emotion`)을 매개변수로 가지며, 해당 감정을 나타내는 문서를 `filtered_texts`에 저장합니다. 이를 사용해서, 입력되는 감정을 표현하는 문서 내 각 단어의 빈도수를 `partial_freq` 딕셔너리 변수에 저장하세요.
2. `cal_total_freq` 함수를 완성하세요. 이 함수는 1번에서 생성된 `partial_freq` 딕셔너리를 입력받아, 특정 감정별 문서 내 전체 단어의 빈도수를 계산하여 반환합니다.
3. `Emotions dataset for NLP` 데이터셋에서 `joy`라는 감정을 표현하는 문장에서 단어 `happy`가 발생할 **가능도**를 `joy_likelihood` 변수에 저장하세요.
4. `Emotions dataset for NLP` 데이터셋에서 `sadness`라는 감정을 표현하는 문장에서 단어 `happy`가 발생할 **가능도**를 `sad_likelihood` 변수에 저장하세요.
5. `Emotions dataset for NLP` 데이터셋에서 `surprise`라는 감정을 표현하는 문장에서 단어 `can`이 발생할 **가능도**를 `sad_likelihood` 변수에 저장하세요.



## 8. 나이브 베이즈 기반 문장 감정 예측

이전 실습을 통해서 나이브 베이즈에서 단어의 가능도를 어떻게 계산하는지 확인하였습니다. 이번에는 나이브 베이즈의 나머지 부분들을 모두 구현하여, 주어진 **문장의 감정**을 예측하도록 하겠습니다.

## 지시사항

1. [실습 2]에서 구현한 `cal_partial_freq`와 `cal_total_freq` 함수를 완성하세요.
2. 입력되는 `data` 내 특정 감정의 **로그 발생 확률** 을 반환해 주는 `cal_prior_prob` 함수를 완성하세요.
   - 로그는 `np.log()` 함수를 사용하여 구할 수 있습니다.
3. 매개변수 `data`를 학습 데이터로 사용하여 `sent`의 각 **감정별 로그 확률**을 계산해 주는 `predict_emotion`함수를 완성하세요. 감정별 로그 확률 계산을 위해 **단어의 로그 가능도**를 사용하세요. 그리고 **스무딩 값을 10**으로 설정해 주세요. 결과는 `(감정, 확률)`의 형태로 `predictions` 리스트에 저장하세요. 이 중 **확률값이 가장 높은 `(감정, 확률)` 튜플**을 반환하세요.
4. `Emotions dataset for NLP 데이터셋`을 학습 데이터로 사용하여, 3번에서 작성한 함수로 `test_sent`의 감정을 예측한 결과를 확인하세요.



## 9. scikit-learn을 통한 나이브 베이즈 감정 분석

지금까지 직접 나이브 베이즈를 구현해봤습니다. 이제 나이브 베이즈의 작동 원리를 파악했으니, `scitkit-learn`을 통해 나이브 베이즈를 보다 간편하게 학습하고 예측하는 방법에 대해 배워보도록 하겠습니다.

`scikit-learn`이란 각종 머신 러닝 모델을 간편하게 사용할 수 있게 도와주는 파이썬 라이브러리입니다.

## 지시사항

1. scikit-learn을 통해 학습 데이터의 문장을 단어의 빈도수 벡터로 생성해 주는 `CountVectorizer` 객체인 변수 `cv`를 만들고, `fit_transform` 메소드로 `train_data`를 변환하세요. 변환 결과를 `transformed_text`에 저장하세요.
   - 학습 문장과 학습 감정은 각각 `train_data`와 `train_emotion`에 저장되어 있습니다.
2. `MultinomialNB` 객체인 변수 `clf`를 생성하고, `fit` 메소드로 2번에서 변환된 `train_data`와 `train_emotion`을 학습하세요.
3. `test_data` 안에 존재하는 5개 문장의 감정을 예측하고, 결과를 `test_result` 변수에 저장하세요. `cv.transform`을 이용해 단어의 빈도수 벡터를 만든 후, `clf.predict`를 이용해 감정을 예측하면 됩니다.



## 10. 나이브 베이즈 기반 감정 분석 서비스

이번 실습에서는 `flask`를 활용하여, 웹 기반으로 나이브 베이즈 모델을 호출하는 방법에 대해 배워보도록 하겠습니다.

`nb_flask.py`에는 `flask`로 작성된 웹 서버의 코드가 포함되어 있습니다. 웹 서버 시작 시, 이전 실습에서 학습된 `CountVectorizer` 객체인 `cv`와 `MultinomialNB` 객체인 `clf`가 로딩됩니다.

## 지시사항

1. `nb_flask.py` 파일 내 `predict` 함수를 완성하세요. 함수 내 `query` 변수는 웹 서버로 전달된 문자열 리스트를 의미합니다. 여기서 기존에 학습된 `CountVectorizer` 객체인 `cv`와 `MultinomialNB` 객체인 `clf`를 사용해서 `query`에 전달된 리스트 내 각 문장의 감정을 예측하는 코드를 작성하세요.
2. 1번에서 예측된 결과를 `response` 딕셔너리에 `문장의 순서: 예측된 감정` 형태로 저장하세요.
3. 실행 버튼을 눌러 `main.py` 내 `test_data`의 감정을 웹 서버를 통해 예측하세요.